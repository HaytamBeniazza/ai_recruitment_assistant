# üîÑ RecruitAI Pro Data Integration Guide

## Overview

RecruitAI Pro supports two main approaches for getting data into the system:

1. **üîó Real Data Integration** - Production-ready integrations with external services
2. **üé≠ Fake Data Generation** - Development and testing with sample data

## üìä Current System Status

**Live Data in System:**
- ‚úÖ **3 Processed Candidates** with AI analysis
- ‚úÖ **5 Active Job Positions** 
- ‚úÖ **8 Communication Messages**
- ‚úÖ **Real-time Analytics** updating automatically

---

## üîó Real Data Integration (Production Approach)

### Available Integration Sources

| Source | Type | Status | Use Case |
|--------|------|---------|----------|
| **Job Boards** | API | ‚úÖ Ready | Indeed, LinkedIn, Glassdoor |
| **Email Processing** | IMAP | ‚úÖ Ready | Resume from email attachments |
| **Cloud Storage** | API | ‚úÖ Ready | AWS S3, Google Cloud, Dropbox |
| **Webhooks** | HTTP | ‚úÖ Ready | Real-time events from ATS |
| **Career Sites** | API | ‚úÖ Ready | Direct applications |

### 1. Job Board Integration

**Supported Platforms:**
- Indeed Jobs API
- LinkedIn Jobs API
- Glassdoor API
- Workday ATS

**Setup Example:**
```python
from integrations.job_boards import job_board_integrator, JobBoardConfig

# Configure job board
config = JobBoardConfig(
    name="indeed",
    api_url="https://api.indeed.com/v1",
    api_key="your_indeed_api_key",
    company_id="your_company_id"
)

# Sync jobs automatically
result = await job_board_integrator.sync_jobs_from_all_boards([config])
```

**Command Line Usage:**
```bash
# Sync jobs from all configured boards
python data_manager.py --mode real --source job_boards --config config.json

# Result: Automatically creates JobPosition records
```

### 2. Email Resume Processing

**Automatically processes:**
- PDF, DOC, DOCX resume attachments
- Extracts candidate information
- Triggers AI analysis
- Creates candidate records

**Setup Example:**
```python
from integrations.resume_sources import EmailConfig

config = EmailConfig(
    server="imap.gmail.com",
    port=993,
    username="hr@yourcompany.com",
    password="your_app_password",
    folder="INBOX"
)

# Process emails automatically
result = await resume_source_integrator.process_email_resumes(config)
```

**Command Line Usage:**
```bash
# Process resume emails
python data_manager.py --mode real --source email --config config.json

# Result: Creates candidate records from email attachments
```

### 3. Cloud Storage Integration

**Supported Platforms:**
- AWS S3
- Google Cloud Storage
- Dropbox
- Azure Blob Storage

**Setup Example:**
```python
from integrations.resume_sources import CloudStorageConfig

config = CloudStorageConfig(
    provider="aws",
    credentials={
        "access_key": "your_aws_access_key",
        "secret_key": "your_aws_secret_key"
    },
    bucket_name="company-resumes",
    folder_path="incoming/"
)

# Process cloud storage files
result = await resume_source_integrator.process_cloud_storage_resumes(config)
```

### 4. Webhook Integration

**Real-time event processing:**
- Job applications
- Status changes
- Interview scheduling
- Form submissions

**Active Endpoints:**
```
POST /webhooks/indeed          # Indeed job board events
POST /webhooks/linkedin        # LinkedIn events
POST /webhooks/workday         # Workday ATS events
POST /webhooks/career-site     # Career website forms
POST /webhooks/generic         # Generic webhook handler
```

**Example Webhook Payload:**
```json
{
  "event_type": "job_application",
  "candidate": {
    "name": "John Doe",
    "email": "john.doe@example.com",
    "phone": "+1234567890"
  },
  "job": {
    "id": "job_123",
    "title": "Software Engineer"
  },
  "resume": {
    "text": "Extracted resume content...",
    "filename": "john_doe_resume.pdf"
  }
}
```

---

## üé≠ Fake Data Generation (Development Approach)

### When to Use Fake Data

‚úÖ **Good for:**
- Development and testing
- Demo environments
- Feature prototyping
- Performance testing
- Training team members

‚ùå **Not suitable for:**
- Production environments
- Real recruitment processes
- Client demonstrations
- Performance benchmarking

### Available Fake Data Types

| Data Type | Count | Content |
|-----------|-------|---------|
| **Job Positions** | 5+ | Realistic job descriptions, requirements, salaries |
| **Candidates** | 10+ | AI-generated profiles, resumes, skills |
| **Interviews** | 3+ | Scheduled interviews with realistic data |
| **Communications** | 8+ | Email templates, message history |

### Command Line Usage

**Generate Sample Data:**
```bash
# Generate 20 fake records
python data_manager.py --mode fake --count 20

# Quick demo data
python backend/sample_data.py

# Upload test resumes
python backend/upload_all_resumes.py
```

**Existing Sample Data Scripts:**
```bash
# Create comprehensive sample data
python backend/create_sample_data.py

# Add specific job positions
python backend/add_sample_data.py

# Test individual components
python backend/test_upload.py
python backend/test_communication.py
```

---

## üîß Configuration

### 1. Create Configuration File

Create `config.json` with your real API credentials:

```json
{
  "job_boards": [
    {
      "name": "indeed",
      "api_url": "https://api.indeed.com/v1",
      "api_key": "your_indeed_api_key",
      "company_id": "your_company_id"
    },
    {
      "name": "linkedin",
      "api_url": "https://api.linkedin.com/v2",
      "api_key": "your_linkedin_api_key",
      "company_id": "your_company_id"
    }
  ],
  "email": {
    "server": "imap.gmail.com",
    "port": 993,
    "username": "hr@yourcompany.com",
    "password": "your_email_app_password",
    "folder": "INBOX",
    "use_ssl": true
  },
  "cloud_storage": {
    "provider": "aws",
    "credentials": {
      "access_key": "your_aws_access_key",
      "secret_key": "your_aws_secret_key",
      "region": "us-east-1"
    },
    "bucket_name": "your-resumes-bucket",
    "folder_path": "incoming/"
  }
}
```

### 2. Environment Variables

Set up `.env` file:
```bash
# API Keys
INDEED_API_KEY=your_indeed_api_key
LINKEDIN_API_KEY=your_linkedin_api_key
OPENAI_API_KEY=your_openai_api_key

# Email Configuration
EMAIL_SERVER=imap.gmail.com
EMAIL_USERNAME=hr@yourcompany.com
EMAIL_PASSWORD=your_app_password

# Cloud Storage
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_REGION=us-east-1
```

---

## üìà Usage Recommendations

### For Development/Testing
```bash
# Start with fake data for development
python data_manager.py --mode fake --count 10

# Check system status
python data_manager.py --mode status

# Test specific components
python backend/test_upload.py
```

### For Production
```bash
# Set up real integrations
python data_manager.py --mode real --source all --config config.json

# Monitor and schedule regular syncs
python data_manager.py --mode real --source job_boards --config config.json

# Process email resumes
python data_manager.py --mode real --source email --config config.json
```

### Hybrid Approach
```bash
# Use fake data for testing, real data for specific features
python data_manager.py --mode fake --count 5  # Base data
python data_manager.py --mode real --source email --config config.json  # Real resumes
```

---

## üöÄ Automation & Scheduling

### Scheduled Data Sync

**Option 1: Cron Jobs**
```bash
# Add to crontab for hourly job sync
0 * * * * cd /path/to/backend && python data_manager.py --mode real --source job_boards --config config.json

# Daily email processing
0 9 * * * cd /path/to/backend && python data_manager.py --mode real --source email --config config.json
```

**Option 2: Python Scheduler**
```python
import schedule
import time

def scheduled_sync():
    """Run scheduled data synchronization"""
    # Sync jobs every hour
    schedule.every().hour.do(sync_job_boards)
    
    # Process emails every 30 minutes
    schedule.every(30).minutes.do(process_email_resumes)
    
    # Cloud storage sync every 6 hours
    schedule.every(6).hours.do(sync_cloud_storage)

    while True:
        schedule.run_pending()
        time.sleep(1)
```

### Webhook Endpoints (Real-time)

Once configured, webhooks provide instant data updates:

- **Job Applications**: Instant candidate creation
- **Status Updates**: Real-time candidate status changes
- **Interview Scheduling**: Automatic calendar integration
- **Form Submissions**: Direct candidate pipeline entry

---

## üéØ Best Practices

### 1. **Start with Fake Data**
- Use fake data for initial development
- Test all features with sample data
- Validate system performance

### 2. **Gradual Real Data Integration**
- Start with one source (e.g., email)
- Validate data quality
- Gradually add more sources

### 3. **Monitor Data Quality**
- Check integration logs regularly
- Validate candidate data accuracy
- Monitor AI analysis results

### 4. **Security**
- Use secure API keys
- Implement webhook signature verification
- Encrypt sensitive data in transit

### 5. **Backup & Recovery**
- Regular database backups
- Test data recovery procedures
- Monitor system health

---

## üìä Current System Performance

**Real Data Processing:**
- **Resume Analysis**: 30-60 seconds per candidate
- **Job Board Sync**: 5-10 jobs per minute
- **Email Processing**: 10-20 resumes per batch
- **Webhook Processing**: < 1 second response time

**Fake Data Generation:**
- **Speed**: 100+ records per minute
- **Quality**: Realistic, varied data
- **Consistency**: Follows real-world patterns

---

## üîç Monitoring & Troubleshooting

### Check System Status
```bash
# Get current system status
python data_manager.py --mode status

# Check API health
curl http://localhost:8000/health

# View integration logs
tail -f logs/integration.log
```

### Common Issues

**1. API Rate Limits**
```bash
# Solution: Configure sync intervals
# Add rate limiting in integration code
```

**2. Email Authentication**
```bash
# Solution: Use app-specific passwords
# Enable IMAP access in email settings
```

**3. File Processing Errors**
```bash
# Solution: Check file formats
# Validate PDF/DOC processing
```

---

## üöÄ Next Steps

1. **Choose Your Approach**: Start with fake data for development, move to real data for production
2. **Configure Integrations**: Set up API keys and credentials
3. **Test Thoroughly**: Validate data quality and system performance
4. **Monitor & Optimize**: Regular checks and performance tuning
5. **Scale Gradually**: Add more data sources as needed

**Questions or Issues?**
- Check the logs in `logs/` directory
- Review API documentation at `http://localhost:8000/docs`
- Test individual components with provided scripts

The system is designed to handle both approaches seamlessly, allowing you to choose the best fit for your specific use case and environment. 